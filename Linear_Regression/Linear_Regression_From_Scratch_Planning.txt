training: 
1) forward pass
2) loss function (cost function)
3) backward pass
4) gradient descent (update weights)

// epoch (cycle) == iterations (10000)

y^ = w*X + b

(w, b)

// regularization (lasso, ridge)       // prevent overfitting, prevent dominance of 1 feature


// multiple linear regression
// y = mx+c
// y = 3*x1 + 1*x2 + 2.5*x3 + c

// mse = (y-y^)^2 + 7amada

 
-forward pass  (calculate prediction)
1) initialize w, b  === 0
2) calculate y^ 

--loss function (use cost function to compare "y^, y")
1) choose a suitable loss functions (loss functions)
: functions calculate ad eh fi error
2) Mean squared error (MSE)
every 1000 epoch print mse

---backward pass (calculate gradient of loss function)
-1/n * 2 * sigma(y-yhat)
-1/n * 2 * sigma(x(y-yhat))

----gradient descent
1) update weights
new weight = old weight - N * gradient